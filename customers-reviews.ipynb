{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://raw.githubusercontent.com/lucasmoratof/customers_review_project/master/reviews_for_nlp.csv',\n",
    "                 index_col=0)\n",
    "df = df.rename(columns={'review_comment_message':'text', 'is_good_review':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_pontuacao(text):\n",
    "    # Remover todas as partes que forem pontuação\n",
    "    clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return clean_text\n",
    "\n",
    "def para_minuscula(text):\n",
    "    # Transforma todo o texto para minúsculas\n",
    "    return text.lower()\n",
    "\n",
    "def remove_line_break(text):\n",
    "    # Remove quebras de linha\n",
    "    return text.replace('\\n', '')\n",
    "\n",
    "def preprocessamento(text):\n",
    "    # Faz todo o pré-processamento do texto\n",
    "    texto_minusculo = para_minuscula(text)\n",
    "    no_line_break_text = remove_line_break(texto_minusculo)\n",
    "    clean_text = remove_pontuacao(no_line_break_text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_text'] = df['text'].apply(preprocessamento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "df['clean_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m spacy download pt_core_news_sm\n",
    "nlp = spacy.load('pt_core_news_sm')\n",
    "\n",
    "def lematizar(texto_original):\n",
    "    # Rodando lematização nas palavras\n",
    "    doc = nlp(texto_original)\n",
    "    \n",
    "    doc_lematizado = [token.lemma_ if token.pos_ in ('VERB', 'AUX') else str(token) for token in doc]\n",
    "\n",
    "    return ' '.join(doc_lematizado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# A lematização é um bom passo de pré-processamento, porém ela demora muito.\n",
    "df['lematized_text'] = df['clean_text'].apply(lematizar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lematized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Agora que o texto já foi pré-processado, podemos usar TF-IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "X = df['lematized_text']\n",
    "y = df['sentiment']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Nota que embora o pré-processamento é feito antes, visto que ele não usa nenhum aprendizado no treino,\n",
    "# o TF-IDF aprende o vocabulário no treino. Então NÃO POSSO usar \"fit\" na base de validação.\n",
    "train_matrix = vectorizer.fit_transform(X_train)\n",
    "test_matrix = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# O SVM também é um modelo que costuma demorar para treinar quando temos muitos dados.\n",
    "from sklearn.svm import SVC\n",
    "model_svm = SVC()\n",
    "model_svm.fit(train_matrix, y_train)\n",
    "predictions = model_svm.predict(test_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_test_array = np.asarray(y_test)\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, predictions))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
